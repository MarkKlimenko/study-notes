= MongoDB notes
:sectnums:
:toc: left
:toclevels: 5
:icons: font
:source-highlighter: coderay

== Introduction

MongoDB is a powerful, flexible, and scalable general-purpose database.
It combines the ability to scale out with features such as secondary indexes, range queries, sorting, aggregations, and geospatial indexes.

MongoDB is a document-oriented database, not a relational one.

A document-oriented database replaces the concept of a “row” with a more flexible model, the “document.” By allowing embedded documents and arrays, the documentoriented approach makes it possible to represent complex hierarchical relationships with a single record.

There are also no predefined schemas: a document’s keys and values are not of fixed types or sizes.
Without a fixed schema, adding or removing fields as needed becomes easier.

=== Designed to Scale

Scaling a database comes down to the choice between scaling up (getting a bigger machine) or scaling out (partitioning data across more machines).

MongoDB was designed to scale out.
The document-oriented data model makes it easier to split data across multiple servers.
MongoDB automatically takes care of balancing data and load across a cluster, redistributing documents automatically and routing reads and writes to the correct machines

image::images/image-2024-05-15-18-10-34-201.png[width=500]

The topology of a MongoDB cluster, or whether there is in fact a cluster rather than a single node at the other end of a database connection, is transparent to the application.
This allows developers to focus on programming the application, not scaling it.

=== Features

** *Indexing*
MongoDB supports generic secondary indexes and provides unique, compound, geospatial, and full-text indexing capabilities as well.
Secondary indexes on hierarchical structures such as nested documents and arrays are also supported and enable developers to take full advantage of the ability to model in ways that best suit their applications.
** *Aggregation*
MongoDB provides an aggregation framework based on the concept of data processing pipelines.
Aggregation pipelines allow you to build complex analytics engines by processing data through a series of relatively simple stages on the server side, taking full advantage of database optimizations.
** *Special collection and index types*
MongoDB supports time-to-live (TTL) collections for data that should expire at a certain time, such as sessions and fixed-size (capped) collections, for holding recent data, such as logs.
MongoDB also supports partial indexes limited to only those documents matching a criteria filter in order to increase efficiency and reduce the amount of storage space required.
** *File storage*
MongoDB supports an easy-to-use protocol for storing large files and file metadata.
** *Limited Joins*
MongoDB supports joins in a very limited way through use of the $lookup aggregation operator

== Getting Started

* A *document* is the basic unit of data for MongoDB and is roughly equivalent to a row in a relational database management system (but much more expressive).
* Similarly, a *collection* can be thought of as a table with a dynamic schema.
* A single instance of MongoDB can host multiple independent *databases*, each of which contains its own collections.
* Every document has a special *key*, "_id", that is unique within a collection.
* MongoDB is distributed with a simple but powerful tool called the *mongo shell*.
The mongo shell provides built-in support for administering MongoDB instances and manipulating data using the MongoDB query language.
* It is also a fully functional *JavaScript interpreter* that enables users to create and load their own scripts for a variety of purposes.

=== Documents

Document: an ordered set of keys with associated values.

[source,json]
----
{"greeting" : "Hello, world!", "views" : 3}
----

=== Collections

A collection is a group of documents.
If a document is the MongoDB analog of a row in a relational database, then a collection can be thought of as the analog to a table.

==== Dynamic Schemas

Collections have dynamic schemas.
This means that the documents within a single collection can have any number of different “shapes.” For example, both of the following documents could be stored in a single collection:

[source,json]
----
{"greeting" : "Hello, world!", "views": 3}
----

[source,json]
----
{"signoff": "Good night, and good luck"}
----

==== Subcollections

One convention for organizing collections is to use namespaced subcollections separated by the . character.
For example, an application containing a blog might have a collection named blog.posts and a separate collection named blog.authors.
This is for organizational purposes only—there is no relationship between the blog collection (it doesn’t even have to exist) and its “children.”

=== Databases

MongoDB groups collections into databases.
A single instance of MongoDB can host several databases, each grouping together zero or more collections.
A good rule of thumb is to store all data for a single application in the same database.

Historically, prior to the use of the WiredTiger storage engine, database names became files on your filesystem.
It is no longer the case.
This explains why many of the previous restrictions exist in the first place.

There are also some reserved database names, which you can access but which have special semantics.
These are as follows:

** *admin*
The admin database plays a role in authentication and authorization.
In addition, access to this database is required for some administrative operations.
See Chapter 19 for more information about the admin database.
** *local*
This database stores data specific to a single server.
In replica sets, local stores data used in the replication process.
The local database itself is never replicated.
(See Chapter 10 for more information about replication and the local database.)
** *config*
Sharded MongoDB clusters (see Chapter 14) use the config database to store information about each shard.

IMPORTANT: By concatenating a database name with a collection in that database you can get a fully qualified collection name, which is called a namespace.
For instance, if you are using the blog.posts collection in the cms database, the namespace of that collection would be cms.blog.posts.
Namespaces are limited to 120 bytes in length and, in practice, should be fewer than 100 bytes long.

== Getting and Starting MongoDB

=== Start server

[source,shell]
----
docker run --name mongo-lessons \
-p 27017:27017 \
--platform linux/arm64/v8 \
mongo:7.0.9

docker start mongo-lessons
----

=== Execute commands

[source,shell]
----
docker exec -it mongo-lessons mongosh
----

=== JavaScript interpreter

[source,shell]
----
> x = 200;
200
> x / 5;
40

> Math.sin(Math.PI / 2);
1
> new Date("20109/1/1");
ISODate("2019-01-01T05:00:00Z")
> "Hello, World!".replace("World", "MongoDB");
Hello, MongoDB!

> function factorial (n) {
... if (n <= 1) return 1;
... return n * factorial(n - 1);
... }
> factorial(5);
120
----

== Basic Commands

To see the database to which db is currently assigned

[source,shell]
----
db
----

Select which database to use

[source,shell]
----
use video
----

=== Create (Insert)

[source,js]
----
// insertOne will add an "_id" key to the document (if you do not supply one) and store the document in MongoDB
movie = {"title" : "Star Wars: Episode IV - A New Hope",
  "director" : "George Lucas",
  "year" : 1977}

db.movies.insertOne(movie)

// or
db.movies.insertOne({"title" : "Stand by Me"})

// insertMany
db.movies.drop()
db.movies.insertMany([{"title" : "Ghostbusters"},
{"title" : "E.T."},
{"title" : "Blade Runner"}])
db.movies.find()
----

If you are just importing raw data (e.g., from a data feed or MySQL), there are command-line tools like mongoimport that can be used instead of a batch insert.

==== Insert Validation

MongoDB does minimal checks on data being inserted: it checks the document’s basic structure and adds an "_id" field if one does not exist.
One of the basic structure checks is size: all documents must be smaller than 16 MB.
This is a somewhat arbitrary limit (and may be raised in the future); it is mostly intended to prevent bad schema design and ensure consistent performance.

IMPORTANT: To give you an idea of how much data 16 MB is, the entire text of War and Peace is just 3.14 MB.

=== Read (findOne)

Will be described later

[source,js]
----
db.movies.findOne()
----

=== Update

updateOne and update Many each take a filter document as their first parameter and a modifier document, which describes changes to make, as the second parameter.
replaceOne also takes a filter as the first parameter, but as the second parameter replaceOne expects a document with which it will replace the document matching the filter.

==== Replace

[source,js]
----
var joe = db.users.findOne({"name" : "joe"});
joe.relationships = {"friends" : joe.friends, "enemies" : joe.enemies};
joe.username = joe.name;

delete joe.friends;
delete joe.enemies;
delete joe.name;

db.users.replaceOne({"name" : "joe"}, joe);
----

==== Update operators

*$inc*

[source,js]
----
/*
{
    "_id" : ObjectId("4b253b067525f35f94b60a31"),
    "url" : "www.example.com",
    "pageviews" : 52
}
*/

db.analytics.updateOne({"url" : "www.example.com"},
{"$inc" : {"pageviews" : 1}})

db.analytics.findOne()
/*
{
    "_id" : ObjectId("4b253b067525f35f94b60a31"),
    "url" : "www.example.com",
    "pageviews" : 53
}
*/
----

*$set $unset*

Sets the value of a field.
If the field does not yet exist, it will be created.
This can be handy for updating schemas or adding user-defined keys.

[source,js]
----
db.movies.updateOne({title : "Star Wars: Episode IV - A New Hope"}, {$set : {reviews: []}})

db.users.updateOne(
    {"_id" : ObjectId("4b253b067525f35f94b60a31")},
    {"$set" : {"favorite book" : "War and Peace"}}
)

// change data type
db.users.updateOne(
    {"name" : "joe"},
    {"$set" : {"favorite book" : ["Cat's Cradle", "Foundation Trilogy", "Ender's Game"]}}
)

// remove field $unset
db.users.updateOne(
    {"name" : "joe"},
    {"$unset" : {"favorite book" : 1}}
)

// You can also use "$set" to reach in and change embedded documents
db.blog.posts.findOne()
/*{
    "_id" : ObjectId("4b253b067525f35f94b60a31"),
    "title" : "A Blog Post",
    "content" : "...",
    "author" : {
        "name" : "joe",
        "email" : "joe@example.com"
    }
}*/
db.blog.posts.updateOne(
    {"author.name" : "joe"},
    {"$set" : {"author.name" : "joe schmoe"}}
)
db.blog.posts.findOne()
/*{
    "_id" : ObjectId("4b253b067525f35f94b60a31"),
    "title" : "A Blog Post",
    "content" : "...",
    "author" : {
        "name" : "joe schmoe",
        "email" : "joe@example.com"
    }
}*/
----

==== Array operators

*$push*

[source,js]
----
/*
{
    "_id" : ObjectId("4b2d75476cc613d5ee930164"),
    "title" : "A blog post",
    "content" : "..."
}
*/

db.blog.posts.updateOne(
    {"title" : "A blog post"},
    {"$push" : {"comments" :
        {"name" : "joe", "email" : "joe@example.com", "content" : "nice post."}
    }}
)

/*
{
    "_id" : ObjectId("4b2d75476cc613d5ee930164"),
    "title" : "A blog post",
    "content" : "...",
    "comments" : [
        {
            "name" : "joe",
            "email" : "joe@example.com",
            "content" : "nice post."
        }
    ]
}
*/

// Now, if we want to add another comment, we can simply use "$push" again
----

*$each*

[source,js]
----
// You can push multiple values in one operation using the "$each" modifier for "$push"

db.stock.ticker.updateOne({"_id" : "GOOG"},
    {"$push" : {"hourly" : {"$each" : [562.776, 562.790, 559.123]}}})

// This would push three new elements onto the array.
----

*$slice*

If you only want the array to grow to a certain length, you can use the "$slice" modifier with "$push" to prevent an array from growing beyond a certain size, effectively making a “top N” list of items:

[source,js]
----
// This example limits the array to the last 10 elements pushed.
// If the array is smaller than 10 elements (after the push), all elements will be kept. If
// the array is larger than 10 elements, only the last 10 elements will be kept.
db.movies.updateOne({"genre" : "horror"},
    {"$push" : {"top10" : {"$each" : ["Nightmare on Elm Street", "Saw"],
    "$slice" : -10}}})
----

*$sort*

Finally, you can apply the "$sort" modifier to "$push" operations before trimming

[source,js]
----
db.movies.updateOne({"genre" : "horror"},
    {"$push" : {"top10" : {"$each" : [{"name" : "Nightmare on Elm Street",
    "rating" : 6.6},
    {"name" : "Saw", "rating" : 4.3}],
    "$slice" : -10,
    "$sort" : {"rating" : -1}}}})
----

This will sort all of the objects in the array by their "rating" field and then keep the first 10.
Note that you must include "$each"; you cannot just "$slice" or "$sort" an array with "$push"

*$ne*

You might want to treat an array as a set, only adding values if they are not present.

[source,js]
----
db.papers.updateOne({"authors cited" : {"$ne" : "Richie"}},
    {$push : {"authors cited" : "Richie"}})

// This can also be done with "$addToSet", which is useful for cases where "$ne" won’t work or where "$addToSet" describes what is happening better.
db.users.updateOne({"_id" : ObjectId("4b2d75476cc613d5ee930164")},
    {"$addToSet" : {"emails" : "joe@gmail.com"}})
----

*$addToSet*

You can also use "$addToSet" in conjunction with "$each" to add multiple unique values, which cannot be done with the "$ne"/"$push" combination.

[source,js]
----
db.users.updateOne({"_id" : ObjectId("4b2d75476cc613d5ee930164")},
    {"$addToSet" : {"emails" : {"$each" :
    ["joe@php.net", "joe@example.com", "joe@python.org"]}}})
----

*$pop*

If you want to treat the array like a queue or a stack, you can use "$pop", which can remove elements from either end. {"$pop" : {"key" : 1}} removes an element from the end of the array. {"$pop" : {"key" : -1}} removes it from the beginning.

*$pull*

Sometimes an element should be removed based on specific criteria, rather than its position in the array. "$pull" is used to remove elements of an array that match the given criteria.

[source,js]
----
db.lists.insertOne({"todo" : ["dishes", "laundry", "dry cleaning"]})

db.lists.updateOne({}, {"$pull" : {"todo" : "laundry"}})

db.lists.findOne()
// {
//     "_id" : ObjectId("4b2d75476cc613d5ee930164"),
//     "todo" : ["dishes", "dry cleaning"]
// }
----

Pulling removes all matching documents, not just a single match.
If you have an array that looks like [1, 1, 2, 1] and pull 1, you’ll end up with a single-element array, [2].

*Positional array modifications*

Arrays use 0-based indexing

[source,js]
----
/*{
  "_id": ObjectId(
  "4b329a216cc613d5ee930192"
  ),
  "content": "...",
  "comments": [
    {
      "comment": "good post",
      "author": "John",
      "votes": 0
    },
    {
      "comment": "i thought it was too short",
      "author": "Claire",
      "votes": 3
    },
    {
      "comment": "free watches",
      "author": "Alice",
      "votes": -5
    },
    {
      "comment": "vacation getaways",
      "author": "Lynn",
      "votes": -7
    }
  ]
}*/

db.blog.updateOne({"post" : post_id},
    {"$inc" : {"comments.0.votes" : 1}})

db.blog.updateOne({"comments.author" : "John"},
... {"$set" : {"comments.$.author" : "Jim"}})
----

==== Upserts

An upsert is a special type of update.
If no document is found that matches the filter, a new document will be created by combining the criteria and updated documents.
If a matching document is found, it will be updated normally.
Upserts can be handy because they can eliminate the need to “seed” your collection: you can often have the same code create and update documents.

[source,js]
----
db.analytics.updateOne({"url" : "/blog"}, {"$inc" : {"pageviews" : 1}},
    {"upsert" : true})

// The new document is created by using the criteria document as a base and applying any modifier documents to it.

db.users.updateOne({"rep" : 25}, {"$inc" : {"rep" : 3}}, {"upsert" : true})
db.users.findOne({"_id" : ObjectId("5727b2a7223502483c7f3acd")} )
// { "_id" : ObjectId("5727b2a7223502483c7f3acd"), "rep" : 28 }
----

==== The save shell helper

save is a shell function that lets you insert a document if it doesn’t exist and update it if it does.
It takes one argument: a document.
If the document contains an "_id" key, save will do an upsert.
Otherwise, it will do an insert. save is really just a convenience function so that programmers can quickly modify documents in the shell:

[source,js]
----
var x = db.testcol.findOne()
x.num = 42
db.testcol.save(x)
----

==== Updating Multiple Documents

So far in this chapter we have used updateOne to illustrate update operations.
updateOne updates only the first document found that matches the filter criteria.
If there are more matching documents, they will remain unchanged.
To modify all of the documents matching a filter, use updateMany. updateMany follows the same semantics as updateOne and takes the same parameters.
The key difference is in the number of documents that might be changed.

[source,js]
----
// Suppose, for example, we want to give a gift to every user who has a birthday on a certain day

db.users.insertMany([
    {birthday: "10/13/1978"},
    {birthday: "10/13/1978"},
    {birthday: "10/13/1978"}])
/*{
"acknowledged" : true,
"insertedIds" : [
    ObjectId("5727d6fc6855a935cb57a65b"),
    ObjectId("5727d6fc6855a935cb57a65c"),
    ObjectId("5727d6fc6855a935cb57a65d")
    ]
}*/

> db.users.updateMany({"birthday" : "10/13/1978"},
    {"$set" : {"gift" : "Happy Birthday!"}})
// { "acknowledged" : true, "matchedCount" : 3, "modifiedCount" : 3 }
----

==== Returning Updated Documents

findOneAndDelete, findOneAndReplace, and findOneAndUpdate

[source,js]
----
db.processes.findOneAndUpdate({"status" : "READY"},
    {"$set" : {"status" : "RUNNING"}},
    {"sort" : {"priority" : -1}})

/*{
"_id" : ObjectId("4b3e7a18005cab32be6291f7"),
"priority" : 1,
"status" : "READY"
}*/
----

Notice that the status is still "READY" in the returned document because the findOneAndUpdate method defaults to returning the state of the document before it was modified.
It will return the updated document if we set the "returnNewDocu ment" field in the options document to true.

=== Delete

[source,js]
----
db.movies.deleteOne({title : "Star Wars: Episode IV - A New Hope"})

// or
db.movies.deleteOne({"_id" : 4})

// or
db.movies.deleteMany({"year" : 1984})

// or
db.movies.drop()
----

Use deleteMany to delete all documents matching a filter

== Data Types

=== Common data types

==== Null

The null type can be used to represent both a null value and a nonexistent field:

{"x" : null}

==== Boolean

There is a boolean type, which can be used for the values true and false:

{"x" : true}

==== Number

The shell defaults to using 64-bit floating-point numbers.
Thus, these numbers both look “normal” in the shell:

{"x" : 3.14}
{"x" : 3}

For integers, use the NumberInt or NumberLong classes, which represent 4-byte or 8-byte signed integers, respectively.

{"x" : NumberInt("3")}
{"x" : NumberLong("3")}

==== String

Any string of UTF-8 characters can be represented using the string type:

{"x" : "foobar"}

==== Date

MongoDB stores dates as 64-bit integers representing milliseconds since the Unix epoch (January 1, 1970).
The time zone is not stored:

{"x" : new Date()}

==== Regular expression

Queries can use regular expressions using JavaScript’s regular expression syntax:

{"x" : /foobar/i}

==== Array

Sets or lists of values can be represented as arrays:

{"x" : ["a", "b", "c"]}

==== Embedded document

Documents can contain entire documents embedded as values in a parent document:

{"x" : {"foo" : "bar"}}

==== Object ID

An object ID is a 12-byte ID for documents:

{"x" : ObjectId()}

See the section “_id and ObjectIds” on page 20 for details.

==== Binary data

Binary data is a string of arbitrary bytes.
It cannot be manipulated from the shell.
Binary data is the only way to save non-UTF-8 strings to the database.

==== Code

MongoDB also makes it possible to store arbitrary JavaScript in queries and documents:

{"x" : function() { /* ... */ }}

== Querying

** *findOne()* returns a document, or nil/null/whatever-it-is-called
** *find()* returns a cursor, which can be empty.
But the object returned is always defined.

=== Introduction to find

The find method is used to perform queries in MongoDB.
Querying returns a subset of documents in a collection, from no documents at all to the entire collection.
Which documents get returned is determined by the first argument to find, which is a document specifying the query criteria.

An empty query document (i.e., {}) matches everything in the collection.
If find isn’t given a query document, it defaults to {}

[source,js]
----
db.c.find()
----

Add search parameters

[source,js]
----
db.users.find({"age" : 27})
db.users.find({"username" : "joe"})
db.users.find({"username" : "joe", "age" : 27})
----

=== Specifying Which Keys to Return

[source,js]
----
// Include
db.users.find({}, {"username" : 1, "email" : 1})
/*{
    "_id" : ObjectId("4ba0f0dfd22aa494fd523620"),
    "username" : "joe",
    "email" : "joe@example.com"
}*/

// Exclude
db.users.find({}, {"fatal_weakness" : 0})
db.users.find({}, {"username" : 1, "_id" : 0})
----

=== Query Criteria

==== Query Conditionals

"$lt", "$lte", "$gt", and "$gte" are all comparison operators, corresponding to <, <=, >, and >=, respectively.

[source,js]
----
// look for users who are between the ages of 18 and 30
db.users.find({"age" : {"$gte" : 18, "$lte" : 30}})

// find people who registered before January 1, 2007
start = new Date("01/01/2007")
db.users.find({"registered" : {"$lt" : start}})

// to find all users who do not have the username “joe”
db.users.find({"username" : {"$ne" : "joe"}})
----

==== OR Queries

[source,js]
----
// "$in" can be used to query for a variety of values for a single key
db.raffle.find({"ticket_no" : {"$in" : [725, 542, 390]}})

// "$in" is very flexible and allows you to specify criteria of different types as well as values
db.users.find({"user_id" : {"$in" : [12345, "joe"]}})

// The opposite of "$in" is "$nin", which returns documents that don’t match any of the criteria in the array.
db.raffle.find({"ticket_no" : {"$nin" : [725, 542, 390]}})

// "$or" takes an array of possible criteria
db.raffle.find({"$or" : [{"ticket_no" : 725}, {"winner" : true}]})

// "$or" can contain other conditionals
db.raffle.find({"$or" : [{"ticket_no" : {"$in" : [725, 542, 390]}},
    {"winner" : true}]})
----

==== $not

"$not" is a metaconditional: it can be applied on top of any other criteria

[source,js]
----
// query returns users with "id_num"s of 1, 6, 11, 16, and so on
db.users.find({"id_num" : {"$mod" : [5, 1]}})

// To return users with "id_num"s of 2, 3, 4, 5, 7, 8, 9, 10, 12, etc., we can use "$not"
db.users.find({"id_num" : {"$not" : {"$mod" : [5, 1]}}})
----

=== Type-Specific Queries

*null*

null behaves a bit strangely.
It does match itself.

[source,js]
----
// if we have a collection with the following documents
db.c.find()
/*{ "_id" : ObjectId("4ba0f0dfd22aa494fd523621"), "y" : null }
{ "_id" : ObjectId("4ba0f0dfd22aa494fd523622"), "y" : 1 }
{ "_id" : ObjectId("4ba0f148d22aa494fd523623"), "y" : 2 }*/

// null also matches “does not exist.” Thus, querying for a key with the value null will return all documents lacking that key
db.c.find({"z" : null})
/*{ "_id" : ObjectId("4ba0f0dfd22aa494fd523621"), "y" : null }
{ "_id" : ObjectId("4ba0f0dfd22aa494fd523622"), "y" : 1 }
{ "_id" : ObjectId("4ba0f148d22aa494fd523623"), "y" : 2 }*/

// If we only want to find keys whose value is null, we can check that the key is null and exists using the "$exists" conditional
db.c.find({"z" : {"$eq" : null, "$exists" : true}})
----

*Regular Expressions*

[source,js]
----
// if we want to find all users with the name “Joe” or “joe,” we can use a regular expression to do case-insensitive matching
db.users.find( {"name" : {"$regex" : /joe/i } })

// Regular expression flags (e.g., i) are allowed but not required
db.users.find({"name" : /joey?/i})
----

MongoDB uses the Perl Compatible Regular Expression (PCRE) library to match regular expressions

==== Querying Arrays

Querying for elements of an array is designed to behave the way querying for scalars does.

[source,js]
----
db.food.insertOne({"fruit" : ["apple", "banana", "peach"]})

// the following query will successfully match the document
db.food.find({"fruit" : "banana"})
----

*$all*

If you need to match arrays by more than one element, you can use "$all".

[source,js]
----
db.food.insertOne({"_id" : 1, "fruit" : ["apple", "banana", "peach"]})
db.food.insertOne({"_id" : 2, "fruit" : ["apple", "kumquat", "orange"]})
db.food.insertOne({"_id" : 3, "fruit" : ["cherry", "banana", "apple"]})

// Then we can find all documents with both "apple" and "banana" elements by querying with "$all"
db.food.find({fruit : {$all : ["apple", "banana"]}})
//{"_id" : 1, "fruit" : ["apple", "banana", "peach"]}
//{"_id" : 3, "fruit" : ["cherry", "banana", "apple"]}

// this will match the first of our three documents
db.food.find({"fruit" : ["apple", "banana", "peach"]})
----

Order does not matter

*$size*

A useful conditional for querying arrays is "$size", which allows you to query for arrays of a given size.

[source,js]
----
db.food.find({"fruit" : {"$size" : 3}})
----

*$slice*

[source,js]
----
db.blog.posts.findOne(criteria, {"comments" : {"$slice" : 10}})
db.blog.posts.findOne(criteria, {"comments" : {"$slice" : [23, 10]}})
----

==== Querying on Embedded Document

[source,js]
----
/*{
    "name" : {
        "first" : "Joe",
        "last" : "Schmoe"
    },
    "age" : 45
}*/

db.people.find({"name" : {"first" : "Joe", "last" : "Schmoe"}})
----

However, a query for a full subdocument must exactly match the subdocument.
If Joe decides to add a middle name field, suddenly this query won’t work anymore; it doesn’t match the entire embedded document!
This type of query is also ordersensitive:
{"last" : "Schmoe", "first" : "Joe"} would not be a match.

[source,js]
----
// If possible, it’s usually a good idea to query for just a specific key or keys of an embedded document.
db.people.find({"name.first" : "Joe", "name.last" : "Schmoe"})
----

==== $where Queries

Key/value pairs are a fairly expressive way to query, but there are some queries that they cannot represent.
For queries that cannot be done any other way, there are "$where" clauses, which allow you to execute arbitrary JavaScript as part of your query.
This allows you to do (almost) anything within a query.
For security, use of "$where" clauses should be highly restricted or eliminated.
End users should never be allowed to execute arbitrary "$where" clauses.

[source,js]
----
db.foo.insertOne({"apple" : 1, "banana" : 6, "peach" : 3})
db.foo.insertOne({"apple" : 8, "spinach" : 4, "watermelon" : 4})

// We’d like to return documents where any two of the fields are equal.
// in the second document, "spinach" and "watermelon" have the same value, so we’d like that document returned

db.foo.find({"$where" : function () {
  for (var current in this) {
    for (var other in this) {
        if (current != other && this[current] == this[other]) {
            return true;
        }
    }
  }
  return false;
}})

// If the function returns true, the document will be part of the result set; if it returns false, it won’t be.
----

"$where" queries should not be used unless strictly necessary: they are much slower than regular queries.
Each document has to be converted from BSON to a JavaScript object and then run through the "$where" expression.
Indexes cannot be used to satisfy a "$where" either.

MongoDB 3.6 added the $expr operator which allows the use of aggregation expressions with the MongoDB query language.
It is faster than $where as it does not execute JavaScript and is recommended as a replacement to this operator where possible.

=== Cursors

The database returns results from find using a cursor.
You can limit the number of results, skip over some number of results, sort results by any combination of keys in any direction, and perform a number of other powerful operations.

To create a cursor with the shell, put some documents into a collection, do a query on them, and assign the results to a local variable

[source,js]
----
for(i=0; i<100; i++) {
 db.collection.insertOne({x : i});
}

var cursor = db.collection.find();
----

If you store the results in a global variable or no variable at all, the MongoDB shell will automatically iterate through and display the first couple of documents.

To iterate through the results, you can use the next method on the cursor.
You can use hasNext to check whether there is another result

[source,js]
----
while (cursor.hasNext()) {
    obj = cursor.next();
    // do stuff
}

// The cursor class also implements JavaScript’s iterator interface, so you can use it in a forEach loop
var cursor = db.people.find();
cursor.forEach(function(x) {
    print(x.name);
});
----

*Chain additional options*

[source,js]
----
var cursor = db.foo.find().sort({"x" : 1}).limit(1).skip(10);
var cursor = db.foo.find().limit(1).sort({"x" : 1}).skip(10);
var cursor = db.foo.find().skip(10).limit(1).sort({"x" : 1});

// At this point, the query has not been executed yet. All of these functions merely build
// the query. Now, suppose we call the following:
cursor.hasNext()
----

At this point, the query will be sent to the server.
The shell fetches the first 100 results or first 4 MB of results (whichever is smaller) at once so that the next calls to next or hasNext will not have to make trips to the server.
After the client has run through the first set of results, the shell will again contact the database and ask for more results with a getMore request. getMore requests basically contain an identifier for the cursor and ask the database if there are any more results, returning the next batch if there are.
This process continues until the cursor is exhausted and all results have been returned.

==== Limits, Skips, and Sorts

[source,js]
----
db.c.find().limit(3)
db.c.find().skip(3)
db.c.find().sort({username : 1, age : -1})

db.stock.find({"desc" : "mp3"}).limit(50).sort({"price" : -1})
db.stock.find({"desc" : "mp3"}).limit(50).skip(50).sort({"price" : -1})
----

Large skips are not very performant; there are suggestions for how to avoid them in the next section.

*Comparison order*

MongoDB has a hierarchy as to how types compare.
Sometimes you will have a single key with multiple types: for instance, integers and booleans, or strings and nulls.
If you do a sort on a key with a mix of types, there is a predefined order that they will be sorted in.
From least to greatest value, this ordering is as follows:

* Minimum value
* Null
* Numbers (integers, longs, doubles, decimals)
* Strings
* Object/document
* Array
* Binary data
* Object ID
* Boolean
* Date
* Timestamp
* Regular expression
* Maximum value

==== Avoiding Large Skips

Using skip for a small number of documents is fine.
But for a large number of results, skip can be slow, since it has to find and then discard all the skipped results.

[source,js]
----
var page1 = db.foo.find().sort({"date" : -1}).limit(100)

var latest = null;
// display first page
while (page1.hasNext()) {
latest = page1.next();
display(latest);
}
// get next page
var page2 = db.foo.find({"date" : {"$lt" : latest.date}});
page2.sort({"date" : -1}).limit(100);
----

==== Finding a random document

The trick is to add an extra random key to each document when it is inserted.

[source,js]
----
db.people.insertOne({"name" : "joe", "random" : Math.random()})
db.people.insertOne({"name" : "john", "random" : Math.random()})
db.people.insertOne({"name" : "jim", "random" : Math.random()})

var random = Math.random()
result = db.people.findOne({"random" : {"$gt" : random}})
----

There is a slight chance that random will be greater than any of the "random" values in the collection, and no results will be returned.
We can guard against this by simply returning a document in the other direction:

[source,js]
----
if (result == null) {
    result = db.people.findOne({"random" : {"$lte" : random}})
}
----

==== Immortal Cursors

There are two sides to a cursor: the client-facing cursor and the database cursor that the client-side one represents.

On the server side, a cursor takes up memory and resources.
Once a cursor runs out of results or the client sends a message telling it to die, the database can free the resources it was using.
Freeing these resources lets the database use them for other things, which is good, so we want to make sure that cursors can be freed quickly (within reason).

There are a couple of conditions that can cause the death (and subsequent cleanup) of a cursor.
First, when a cursor finishes iterating through the matching results, it will clean itself up.
Another way is that, when a cursor goes out of scope on the client side, the drivers send the database a special message to let it know that it can kill that cursor.

This “death by timeout” is usually the desired behavior: very few applications expect their users to sit around for minutes at a time waiting for results.
However, sometimes you might know that you need a cursor to last for a long time.
In that case, many drivers have implemented a function called immortal, or a similar mechanism, which tells the database not to time out the cursor.
If you turn off a cursor’s timeout, you must iterate through all of its results or kill it to make sure it gets closed.
Otherwise, it will sit around in the database hogging resources until the server is restarted.

== Indexes

=== Creating an Index

[source,js]
----
db.users.createIndex({"username" : 1})
db.users.createIndex({"age" : 1, "username" : 1})
----

**In general you should design compound indexes such that fields on which you will be using equality filters come before those on which your application will use multivalue filters.**

To use the index to *sort*, MongoDB needs to be able to walk the index keys in order.
This means that we need to include the sort field among the compound index keys.

Note that we include the sort component immediately after the equality filter, but before the multivalue filter.

To recap, when designing a compound index:
• Keys for equality filters should appear first.
• Keys used for sorting should appear before multivalue fields.
• Keys for multivalue filters should appear last.

Example:

[source,js]
----
db.students.find({student_id:{$gt:500000}, class_id:54})
    .sort({final_grade:1})
    .explain("executionStats")

db.students.createIndex({class_id:1, final_grade:1, student_id:1})
----

==== Rules for SQL and noSQL

[source,sql]
----
WHERE x = 1
AND y > 2
----

The relevant characteristics are:

* x and y are in the same table.
(Can't build an index across two tables.)
* AND is used.
(OR can't be optimized.)
* One of the tests is with =.
(Composite won't help if both are ranges.)
* y is a "range" (examples: y>2, y LIKE 'm%', y BETWEEN ... AND ...).

*The general rule is:*

* Put all the = columns first (x in my example)
* Put one range column last (y)
* That is, you must order it INDEX(x,y).

IMPORTANT: For WHERE x = 1 AND y = 2 (both =), it does not matter whether you have INDEX(x,y) or INDEX(y,x) .

==== Choosing key directions

To optimize compound sorts in different directions, we need to use an index with matching directions.
In this example, we could use `{"age" : 1, "username" : -1}`

Note that inverse indexes (multiplying each direction by −1) are equivalent: `{"age" : 1, "username" : -1}` suits the same queries that `{"age" : -1, "username" : 1}`

Index direction only really matters when you’re sorting based on multiple criteria.

==== Using covered queries

In the preceding examples, the index was always used to find the correct document and then follow a pointer back to fetch the actual document.
However, if your query is only looking for the fields that are included in the index, it does not need to fetch the document.
When an index contains all the values requested by a query, the query is considered to be covered.
Whenever practical, use covered queries in preference to going back to documents.
You can make your working set much smaller that way.

==== Implicit indexes

If we have an index on {"age" : 1, "username" : 1}, the "age" field is sorted identically to the way it would be if we had an index on just {"age" : 1}.
Thus, the compound index can be used the way an index on {"age" : 1} by itself would be.

This can be generalized to as many keys as necessary: if an index has N keys, you get a “free” index on any prefix of those keys.
For example, if we have an index that looks like {"a": 1, "b": 1, "c": 1, ..., "z": 1}, we effectively have indexes on
{"a": 1}, {"a": 1, "b" : 1}, {"a": 1, "b": 1, "c": 1}, and so on.

Note that this doesn’t hold for any subset of keys: queries that would use the index
{"b": 1} or {"a": 1, "c": 1} (for example) will not be optimized.
Only queries that can use a prefix of the index can take advantage of it.

=== Indexing Objects and Arrays

[source,js]
----
//{
//    "username" : "sid",
//    "loc" : {
//        "ip" : "1.2.3.4",
//        "city" : "Springfield",
//        "state" : "NY"
//    }
//}

db.users.createIndex({"loc.city" : 1})

// index Array
db.blog.createIndex({"comments.10.votes": 1})
----

=== Explain Output

explain gives you lots of information about your queries.
It’s one of the most important diagnostic tools there is for slow queries.

For any query, you can add a call to explain at the end (the way you would add a sort or limit, but explain must be the last call).

There are two types of explain output that you’ll see most commonly: for indexed and nonindexed queries.
Special index types may create slightly different query plans, but most fields should be similar.
Also, sharding returns a conglomerate of explains, as it runs the query on multiple servers.

The most basic type of explain is on a query that doesn’t use an index.
You can tell that a query doesn’t use an index because it uses a "COLLSCAN".

*see explain Output in Chapter 5: Indexes*

=== Types of Indexes

==== Unique Indexes

[source,js]
----
db.users.createIndex({"firstname" : 1},
    {"unique" : true, "partialFilterExpression":{
    "firstname": {$exists: true } } } )
----

==== Partial Indexes

If you have a field that may or may not exist but must be unique when it does, you can combine the "unique" option with the "partial" option.

=== Index Administration

All of the information about a database’s indexes is stored in the system.indexes collection.
This is a reserved collection, so you cannot modify its documents or remove documents from it.
You can manipulate it only through the createIndex, createIn dexes, and dropIndexes database commands.

When you create an index, you can see its metainformation in system.indexes.
You can also run db.collectionName.getIndexes() to see information about all the indexes on a given collection

==== Identifying Indexes

[source,js]
----
db.soup.createIndex({"a" : 1, "b" : 1, "c" : 1, ..., "z" : 1},
... {"name" : "alphabet"})
----

==== Changing Indexes

[source,js]
----
db.people.dropIndex("x_1_y_1")
----

== Special Index and Collection Types

This chapter covers the special collections and index types MongoDB has available, including:

* Capped collections for queue-like data
* TTL indexes for caches
* Full-text indexes for simple string searching
* Geospatial indexes for 2D and spherical geometries
* GridFS for storing large files

=== Geospatial Indexes

MongoDB has two types of geospatial indexes: 2dsphere and 2d. 2dsphere indexes work with spherical geometries that model the surface of the earth based on the WGS84 datum.
This datum models the surface of the earth as an oblate spheroid, meaning that there is some flattening at the poles.
Distance calculations using 2sphere indexes, therefore, take the shape of the earth into account and provide a more accurate treatment of distance between, for example, two cities, than do 2d indexes.
Use 2d indexes for points stored on a two-dimensional plane.

A point is given by a two-element array, representing [longitude, latitude]

[source,json]
----
{
    "name" : "New York City",
    "loc" : {
        "type" : "Point",
        "coordinates" : [50, 2]
    }
}

{
  "name" : "Hudson River",
  "loc" : {
    "type" : "LineString",
    "coordinates" : [[0,1], [0,2], [1,2]]
  }
}
----

You can create a geospatial index using the "2dsphere" type with createIndex:

[source,js]
----
db.openStreetMap.createIndex({"loc" : "2dsphere"})
----

==== Types of Geospatial Queries

intersection, within, and nearness

=== Indexes for Full Text Search

text indexes in MongoDB support full-text search requirements.
This type of text index should not be confused with the MongoDB Atlas Full-Text Search Indexes, which utilize Apache Lucene for additional text search capabilities when compared to MongoDB text indexes.
Use a text index if your application needs to enable users to submit keyword queries that should match titles, descriptions, and text in other fields within a collection.

==== Creating a Text Index

[source,js]
----
db.articles.createIndex({"title": "text",
                        "body" : "text"})
----

You can control the relative importance MongoDB attaches to each field by specifying weights

[source,js]
----
db.articles.createIndex({"title": "text","body": "text"},
        {"weights" : {"title" : 3,"body" : 2}}
)
----

For some collections, you may not know which fields a document will contain.
You can create a full-text index on all string fields in a document by creating an index on "$**"—this not only indexes all top-level string fields, but also searches embedded documents and arrays for string fields:

[source,js]
----
db.articles.createIndex({"$**" : "text"})
----

==== Text Search

Use the "$text" query operator to perform text searches on a collection with a text index.

[source,js]
----
db.articles.find({"$text": {"$search": "impact crater lunar"}},
    {title: 1}
    ).limit(10)

/*{ "_id" : "170375", "title" : "Chengdu" }
{ "_id" : "34331213", "title" : "Avengers vs. X-Men" }
{ "_id" : "498834", "title" : "Culture of Tunisia" }
{ "_id" : "602564", "title" : "ABC Warriors" }
{ "_id" : "40255", "title" : "Jupiter (mythology)" }
{ "_id" : "80356", "title" : "History of Vietnam" }*/
----

==== Optimizing Full-Text Search

There are a couple of ways to optimize full-text searches.
If you can first narrow your search results by other criteria, you can create a compound index with a prefix of those criteria and then the full-text fields:

[source,js]
----
db.blog.createIndex({"date" : 1, "post" : "text"})
----

=== Capped Collections

“Normal” collections in MongoDB are created dynamically and automatically grow in size to fit additional data.
MongoDB also supports a different type of collection, called a capped collection, which is created in advance and is fixed in size

[source,js]
----
db.createCollection("my_collection", {"capped" : true, "size" : 100000});
----

=== Time-To-Live Indexes

If you need a more flexible age-out system, TTL indexes allow you to set a timeout for each document.

[source,js]
----
// 24-hour timeout
db.sessions.createIndex({"lastUpdated" : 1}, {"expireAfterSeconds" : 60*60*24})
----

This creates a TTL index on the "lastUpdated" field.
If a document’s "lastUpdated" field exists and is a date, the document will be removed once the server time is "expireAfterSeconds" seconds ahead of the document’s time.

To prevent an active session from being removed, you can update the "lastUpdated" field to the current time whenever there is activity.
Once "lastUpdated" is 24 hours old, the document will be removed.

=== Storing Files with GridFS

GridFS is a mechanism for storing large binary files in MongoDB.
There are several reasons why you might consider using GridFS for file storage:

* Using GridFS can simplify your stack.
If you’re already using MongoDB, you might be able to use GridFS instead of a separate tool for file storage.
* GridFS will leverage any existing replication or autosharding that you’ve set up for MongoDB, so getting failover and scale-out for file storage is easier.
* GridFS can alleviate some of the issues that certain filesystems can exhibit when being used to store user uploads.
For example, GridFS does not have issues with storing large numbers of files in the same directory.

There are some downsides, too:

* Performance is slower.
Accessing files from MongoDB will not be as fast as going directly through the filesystem.
* You can only modify documents by deleting them and resaving the whole thing.
MongoDB stores files as multiple documents, so it cannot lock all of the chunks in a file at the same time.

*look at Chapter 6: Special Index and Collection Types for more information*

== Aggregation Framework

MongoDB provides powerful support for running analytics natively using the aggregation framework.

The aggregation framework is based on the concept of a pipeline.

[source,js]
----
db.companies.aggregate([{$match: {founded_year: 2004}}])
// or
db.companies.find({founded_year: 2004})

// add specific fields
db.companies.aggregate([{$match: {founded_year: 2004}},
    {$project: {
        _id: 0,
        name: 1,
        founded_year: 1
    }}
])

// add limit
db.companies.aggregate([
    {$match: {founded_year: 2004}},
    {$limit: 5},
    {$project: {
        _id: 0,
        name: 1}}
])

// add sort
db.companies.aggregate([
    { $match: { founded_year: 2004 } },
    { $sort: { name: 1} },
    { $limit: 5 },
    { $project: {
        _id: 0,
        name: 1 } }
])

// add skip
db.companies.aggregate([
    {$match: {founded_year: 2004}},
    {$sort: {name: 1}},
    {$skip: 10},
    {$limit: 5},
    {$project: {
        _id: 0,
        name: 1}},
])
----

=== $project

Promotes required fields

[source,js]
----
db.companies.aggregate([
{$match: {"funding_rounds.investments.financial_org.permalink": "greylock" }},
{$project: {
    _id: 0,
    name: 1,
    ipo: "$ipo.pub_year",
    valuation: "$ipo.valuation_amount",
    funders: "$funding_rounds.investments.financial_org.permalink"
    }}
]).pretty()
----

=== $unwind
=== Array Expressions
=== Accumulators

[source,js]
----
db.companies.aggregate([
    { $match: { "funding_rounds": { $exists: true, $ne: [ ]} } },
    { $project: {
        _id: 0,
        name: 1,
        largest_round: { $max: "$funding_rounds.raised_amount" }
    }}
])
----

=== Grouping

[source,js]
----
db.companies.aggregate([
    { $group: {
        _id: { founded_year: "$founded_year" },
        average_number_of_employees: { $avg: "$number_of_employees" }
    }},
    { $sort: { average_number_of_employees: -1 } }
])
----

== Transactions

